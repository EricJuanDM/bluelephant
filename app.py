import streamlit as st
import time 

# --- IMPORTS DAS CLASSES REAIS DO CORE (Essenciais para o funcionamento) ---
from core.llm_agent import LLMAgent 
from core.vector_store import VectorStoreManager 

# ---------------------- CONFIGURA√á√ÉO E INICIALIZA√á√ÉO ----------------------

st.set_page_config(page_title="Agente de IA com Feedback Inteligente", layout="wide")
st.title("ü§ñ Chatbot Inteligente com Melhoria de Prompt em Tempo Real")
st.markdown("---")

@st.cache_resource
def initialize_system():
    # 1. Inicializa o Vector Store Manager (ChromaDB)
    try:
        vs_manager = VectorStoreManager()
        vs_manager.initialize_static_data() 
    except Exception as e:
        st.error(f"Erro ao inicializar Vector Store (ChromaDB). Verifique o volume Docker: {e}")
        return None
    
    # 2. Inicializa o Agente e passa o Vector Store Manager
    try:
        agent = LLMAgent(vector_store_manager=vs_manager)
    except Exception as e:
        st.error(f"Erro ao inicializar LLM Agent. Chave Gemini API configurada? Erro: {e}")
        return None
        
    return agent

agent = initialize_system()

if agent is None:
    st.warning("O sistema de IA n√£o p√¥de ser iniciado. Por favor, corrija os erros acima e reinicie.")
    st.stop()


# ---------------------- GEST√ÉO DO ESTADO DA SESS√ÉO ----------------------

if "messages" not in st.session_state:
    st.session_state.messages = []

if "last_interaction" not in st.session_state:
    st.session_state.last_interaction = None

# ---------------------- DEFINI√á√ÉO DAS ABAS ----------------------

tab_chat, tab_feedback = st.tabs(["üí¨ Chat do Agente", "üìù Feedback e Melhoria"])

# ==============================================================================
# --- √ÅREA 1: CHAT DO AGENTE ---
# ==============================================================================
with tab_chat:
    st.header("Converse com o Agente")
    
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    prompt = st.chat_input("Pergunte algo ou solicite uma a√ß√£o.")

    if prompt:
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.spinner("ü§ñ O agente est√° processando a resposta e buscando informa√ß√µes..."):
            try:
                # O ERRO OCORRIA AQUI! Se o LLMAgent n√£o tivesse o m√©todo process_query.
                agent_response = agent.process_query(prompt)

                st.session_state.last_interaction = {
                    "query": prompt,
                    "response": agent_response
                }

            except Exception as e:
                # Captura qualquer falha no processamento do LLM ou Tools
                agent_response = f"Erro no Agente: Falha ao processar a pergunta. Detalhes: {e}"
                st.error(agent_response)
                
        with st.chat_message("assistant"):
            st.markdown(agent_response)
            st.session_state.messages.append({"role": "assistant", "content": agent_response})


# ==============================================================================
# --- √ÅREA 2: FEEDBACK E MELHORIA ---
# ==============================================================================
with tab_feedback:
    st.header("Avalie e Sugira Melhorias para o Agente")
    
    st.subheader("1. Fornecer Feedback sobre a √öltima Resposta")

    interaction = st.session_state.last_interaction
    
    if interaction:
        st.info(f"√öltima Pergunta: **{interaction['query']}**")
        st.info(f"√öltima Resposta: **{interaction['response'][:150]}...**")
    else:
        st.warning("Interaja no chat primeiro para gerar feedback.")
        

    feedback_quality = st.radio(
        "Qualidade da Resposta:",
        ('Excelente (5/5)', 'Boa (4/5)', 'Razo√°vel (3/5)', 'Ruim (2/5)', 'Incorreta (1/5)'),
        index=None,
        horizontal=True
    )
    
    feedback_suggestion = st.text_area(
        "Sugest√µes de Melhoria (Obrigat√≥rio para notas baixas):",
        placeholder="A resposta estava incompleta, o agente deveria ter usado a ViaCEP para a resposta.",
        height=100
    )

    def handle_feedback():
        if interaction and feedback_quality and feedback_suggestion:
            with st.spinner("üß† Processando feedback e gerando novo prompt..."):
                try:
                    success, message = agent.update_prompt_from_feedback(
                        query=interaction['query'],
                        response=interaction['response'],
                        rating=feedback_quality,
                        suggestion=feedback_suggestion
                    )
                except Exception as e:
                    st.error(f"‚ùå Falha no LLM Otimizador: {e}")
                    return
            
            if success:
                st.success(f"‚úÖ Sucesso! {message}")
                st.session_state.last_interaction = None 
            else:
                st.info(f"‚ÑπÔ∏è Tentativa conclu√≠da. {message}")
        else:
            st.error("Por favor, selecione a qualidade e insira uma sugest√£o de melhoria.")

    st.button(
        "Enviar Feedback e Atualizar Prompt", 
        on_click=handle_feedback, 
        type="primary", 
        disabled=(interaction is None)
    )

    st.markdown("---")

    st.subheader("2. Status e Hist√≥rico de Prompts")
    
    st.text(f"Vers√£o Atual: {len(agent.prompt_history)}")
    st.code(agent.current_prompt, language="markdown")
    
    with st.expander(f"Hist√≥rico de {len(agent.prompt_history)} Vers√µes de Prompt"):
        for entry in reversed(agent.prompt_history):
            st.markdown(f"**Vers√£o {entry['version']}** - *Fonte: {entry['source']}*")
            st.code(entry['prompt'], language="markdown")
            if 'based_on_feedback' in entry:
                st.caption(f"Motiva√ß√£o: '{entry['based_on_feedback']['suggestion']}'")
            st.divider()